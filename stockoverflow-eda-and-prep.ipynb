{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-27T19:00:39.780132Z","iopub.execute_input":"2023-09-27T19:00:39.780673Z","iopub.status.idle":"2023-09-27T19:00:39.792947Z","shell.execute_reply.started":"2023-09-27T19:00:39.780617Z","shell.execute_reply":"2023-09-27T19:00:39.791017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the dependencies","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport sqlite3\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom wordcloud import WordCloud\nimport re\nimport os\nfrom sqlalchemy import create_engine # database connection\nimport datetime as dt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom skmultilearn.adapt import mlknn\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom skmultilearn.problem_transform import LabelPowerset\nfrom sklearn.naive_bayes import GaussianNB\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:00:39.795475Z","iopub.execute_input":"2023-09-27T19:00:39.796818Z","iopub.status.idle":"2023-09-27T19:00:39.813275Z","shell.execute_reply.started":"2023-09-27T19:00:39.796765Z","shell.execute_reply":"2023-09-27T19:00:39.810844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> 1. Exploratory Data Analysis </h1>\n\n<h2> 1.1 Data Loading and Cleaning </h2>\n\n<h3>1.1.1 Using Pandas with SQLite to Load the data</h3>","metadata":{}},{"cell_type":"code","source":"if not os.path.isfile('train.db'):\n    start = datetime.now()\n    disk_engine = create_engine('sqlite:///train.db')\n    start = dt.datetime.now()\n    chunksize = 180000\n    j = 0\n    index_start = 1\n    for df in pd.read_csv('/kaggle/input/facebook-recruiting-iii-keyword-extraction/Train.zip', names=['Id', 'Title', 'Body', 'Tags'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n        df.index += index_start\n        j+=1\n        print('{} rows'.format(j*chunksize))\n        df.to_sql('data', disk_engine, if_exists='append')\n        index_start = df.index[-1] + 1\n    print(\"Time taken to run this cell :\", datetime.now() - start)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:00:39.815839Z","iopub.execute_input":"2023-09-27T19:00:39.816238Z","iopub.status.idle":"2023-09-27T19:00:39.828456Z","shell.execute_reply.started":"2023-09-27T19:00:39.816203Z","shell.execute_reply":"2023-09-27T19:00:39.827500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> 1.1.2 Counting the number of rows </h3>","metadata":{}},{"cell_type":"code","source":"if os.path.isfile('train.db'):\n    start = datetime.now()\n    con = sqlite3.connect('train.db')\n    num_rows = pd.read_sql_query(\"\"\"SELECT count(*) FROM data\"\"\", con)\n    #Always remember to close the database\n    print(\"Number of rows in the database :\",\"\\n\",num_rows['count(*)'].values[0])\n    con.close()\n    print(\"Time taken to count the number of rows :\", datetime.now() - start)\nelse:\n    print(\"Please download the train.db file from drive or run the above cell to genarate train.db file\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:00:40.029311Z","iopub.execute_input":"2023-09-27T19:00:40.029804Z","iopub.status.idle":"2023-09-27T19:00:59.212496Z","shell.execute_reply.started":"2023-09-27T19:00:40.029764Z","shell.execute_reply":"2023-09-27T19:00:59.211549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>1.1.3 Checking for duplicates </h3>","metadata":{}},{"cell_type":"code","source":"if os.path.isfile('train.db'):\n    start = datetime.now()\n    con = sqlite3.connect('train.db')\n    df_no_dup = pd.read_sql_query('SELECT Title, Body, Tags, COUNT(*) as cnt_dup FROM data GROUP BY Title, Body, Tags', con)\n    con.close()\n    print(\"Time taken to run this cell :\", datetime.now() - start)\nelse:\n    print(\"Please download the train.db file from drive or run the first to genarate train.db file\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:00:59.216045Z","iopub.execute_input":"2023-09-27T19:00:59.216859Z","iopub.status.idle":"2023-09-27T19:04:01.314467Z","shell.execute_reply.started":"2023-09-27T19:00:59.216802Z","shell.execute_reply":"2023-09-27T19:04:01.313397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_dup.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:01.316523Z","iopub.execute_input":"2023-09-27T19:04:01.316951Z","iopub.status.idle":"2023-09-27T19:04:01.334592Z","shell.execute_reply.started":"2023-09-27T19:04:01.316914Z","shell.execute_reply":"2023-09-27T19:04:01.333094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of duplicate questions :\", num_rows['count(*)'].values[0]- df_no_dup.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:01.336577Z","iopub.execute_input":"2023-09-27T19:04:01.338237Z","iopub.status.idle":"2023-09-27T19:04:01.352965Z","shell.execute_reply.started":"2023-09-27T19:04:01.338178Z","shell.execute_reply":"2023-09-27T19:04:01.351370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of times each question appeared in our database\ndf_no_dup.cnt_dup.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:01.359089Z","iopub.execute_input":"2023-09-27T19:04:01.359588Z","iopub.status.idle":"2023-09-27T19:04:01.414564Z","shell.execute_reply.started":"2023-09-27T19:04:01.359547Z","shell.execute_reply":"2023-09-27T19:04:01.413053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = datetime.now()\n# Check for and handle None values in the 'Tags' column\ndf_no_dup['Tags'] = df_no_dup['Tags'].fillna('')  # Replace None with an empty string\ndf_no_dup[\"tag_count\"] = df_no_dup[\"Tags\"].apply(lambda text: len(text.split(\" \")))\n# adding a new feature number of tags per question\nprint(\"Time taken to run this cell :\", datetime.now() - start)\ndf_no_dup.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:01.416502Z","iopub.execute_input":"2023-09-27T19:04:01.417030Z","iopub.status.idle":"2023-09-27T19:04:07.401280Z","shell.execute_reply.started":"2023-09-27T19:04:01.416981Z","shell.execute_reply":"2023-09-27T19:04:07.399161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of number of tags per question\ndf_no_dup.tag_count.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:07.402909Z","iopub.execute_input":"2023-09-27T19:04:07.403297Z","iopub.status.idle":"2023-09-27T19:04:07.459137Z","shell.execute_reply.started":"2023-09-27T19:04:07.403264Z","shell.execute_reply":"2023-09-27T19:04:07.456997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a new database with no duplicates\nif not os.path.isfile('train_no_dup.db'):\n    disk_dup = create_engine(\"sqlite:///train_no_dup.db\")\n    no_dup = pd.DataFrame(df_no_dup, columns=['Title', 'Body', 'Tags'])\n    no_dup.to_sql('no_dup_train',disk_dup)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:07.463308Z","iopub.execute_input":"2023-09-27T19:04:07.463783Z","iopub.status.idle":"2023-09-27T19:04:07.477647Z","shell.execute_reply.started":"2023-09-27T19:04:07.463748Z","shell.execute_reply":"2023-09-27T19:04:07.476006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This method seems more appropriate to work with this much data.\n#creating the connection with database file.\nif os.path.isfile('train_no_dup.db'):\n    start = datetime.now()\n    con = sqlite3.connect('train_no_dup.db')\n    tag_data = pd.read_sql_query(\"\"\"SELECT Tags FROM no_dup_train\"\"\", con)\n    #Always remember to close the database\n    con.close()\n\n    # Let's now drop unwanted column.\n    tag_data.drop(tag_data.index[0], inplace=True)\n    #Printing first 5 columns from our data frame\n    tag_data.head()\n    print(\"Time taken to run this cell :\", datetime.now() - start)\nelse:\n    print(\"Please download the train.db file from drive or run the above cells to genarate train.db file\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:04:07.480425Z","iopub.execute_input":"2023-09-27T19:04:07.480917Z","iopub.status.idle":"2023-09-27T19:05:07.120398Z","shell.execute_reply.started":"2023-09-27T19:04:07.480878Z","shell.execute_reply":"2023-09-27T19:05:07.118670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 2.2 Analysis of Tags </h2>\n\n<h3> 2.2.1 Total number of unique tags </h3>","metadata":{}},{"cell_type":"code","source":"# Importing & Initializing the \"CountVectorizer\" object, which \n#is scikit-learn's bag of words tool.\n\n\nvectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of strings.\ntag_dtm = vectorizer.fit_transform(tag_data['Tags'])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:07.122569Z","iopub.execute_input":"2023-09-27T19:05:07.123176Z","iopub.status.idle":"2023-09-27T19:05:37.247700Z","shell.execute_reply.started":"2023-09-27T19:05:07.123099Z","shell.execute_reply":"2023-09-27T19:05:37.246650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of data points :', tag_dtm.shape[0])\nprint('Number of unique tags :', tag_dtm.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.249365Z","iopub.execute_input":"2023-09-27T19:05:37.250043Z","iopub.status.idle":"2023-09-27T19:05:37.256124Z","shell.execute_reply.started":"2023-09-27T19:05:37.250009Z","shell.execute_reply":"2023-09-27T19:05:37.254960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags=vectorizer.get_feature_names_out()\nprint(tags[:10])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.258054Z","iopub.execute_input":"2023-09-27T19:05:37.258541Z","iopub.status.idle":"2023-09-27T19:05:37.314088Z","shell.execute_reply.started":"2023-09-27T19:05:37.258491Z","shell.execute_reply":"2023-09-27T19:05:37.312591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> 2.2.3 Number of times a tag appeared </h3>","metadata":{}},{"cell_type":"code","source":"freq=tag_dtm.sum(axis=0).A1\nresult=dict(zip(tags,freq))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.316226Z","iopub.execute_input":"2023-09-27T19:05:37.317539Z","iopub.status.idle":"2023-09-27T19:05:37.424172Z","shell.execute_reply.started":"2023-09-27T19:05:37.317491Z","shell.execute_reply":"2023-09-27T19:05:37.423200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.isfile('tag_counts_dict_dtm.csv'):\n    with open('tag_counts_dict_dtm.csv','w') as csv_file:\n        writer=csv.writer(csv_file)\n        for key, value in result.items():\n            writer.writerow([key,value])\ntag_df=pd.read_csv('tag_counts_dict_dtm.csv',names=['Tags','Counts'])\ntag_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.429612Z","iopub.execute_input":"2023-09-27T19:05:37.430675Z","iopub.status.idle":"2023-09-27T19:05:37.482910Z","shell.execute_reply.started":"2023-09-27T19:05:37.430604Z","shell.execute_reply":"2023-09-27T19:05:37.481675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_df_sorted=tag_df.sort_values(['Counts'],ascending=False)\ntag_counts=tag_df_sorted['Counts'].values","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.484530Z","iopub.execute_input":"2023-09-27T19:05:37.484916Z","iopub.status.idle":"2023-09-27T19:05:37.500109Z","shell.execute_reply.started":"2023-09-27T19:05:37.484884Z","shell.execute_reply":"2023-09-27T19:05:37.498698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(tag_counts)\nplt.title(\"Distribution of number of times tag appeared questions\")\nplt.grid()\nplt.xlabel(\"Tag number\")\nplt.ylabel(\"Number of times tag appeared\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.501913Z","iopub.execute_input":"2023-09-27T19:05:37.502310Z","iopub.status.idle":"2023-09-27T19:05:37.880044Z","shell.execute_reply.started":"2023-09-27T19:05:37.502278Z","shell.execute_reply":"2023-09-27T19:05:37.878207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(tag_counts[0:10000])\nplt.title('first 10k tags: Distribution of number of times tag appeared questions')\nplt.grid()\nplt.xlabel(\"Tag number\")\nplt.ylabel(\"Number of times tag appeared\")\nplt.show()\nprint(len(tag_counts[0:10000:25]), tag_counts[0:10000:25])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:37.881358Z","iopub.execute_input":"2023-09-27T19:05:37.881719Z","iopub.status.idle":"2023-09-27T19:05:38.273599Z","shell.execute_reply.started":"2023-09-27T19:05:37.881688Z","shell.execute_reply":"2023-09-27T19:05:38.272387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(tag_counts[0:1000])\nplt.title('first 1k tags: Distribution of number of times tag appeared questions')\nplt.grid()\nplt.xlabel(\"Tag number\")\nplt.ylabel(\"Number of times tag appeared\")\nplt.show()\nprint(len(tag_counts[0:1000:5]), tag_counts[0:1000:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:38.275480Z","iopub.execute_input":"2023-09-27T19:05:38.275965Z","iopub.status.idle":"2023-09-27T19:05:38.670434Z","shell.execute_reply.started":"2023-09-27T19:05:38.275932Z","shell.execute_reply":"2023-09-27T19:05:38.669177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(tag_counts[0:500])\nplt.title('first 500 tags: Distribution of number of times tag appeared questions')\nplt.grid()\nplt.xlabel(\"Tag number\")\nplt.ylabel(\"Number of times tag appeared\")\nplt.show()\nprint(len(tag_counts[0:500:5]), tag_counts[0:500:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:38.672219Z","iopub.execute_input":"2023-09-27T19:05:38.672604Z","iopub.status.idle":"2023-09-27T19:05:39.070507Z","shell.execute_reply.started":"2023-09-27T19:05:38.672572Z","shell.execute_reply":"2023-09-27T19:05:39.069226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(tag_counts[0:100], c='b')\nplt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label=\"quantiles with 0.05 intervals\")\n# quantiles with 0.25 difference\nplt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = \"quantiles with 0.25 intervals\")\n\nfor x,y in zip(list(range(0,100,25)), tag_counts[0:100:25]):\n    plt.annotate(\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))\n\nplt.title('first 100 tags: Distribution of number of times tag appeared questions')\nplt.grid()\nplt.xlabel(\"Tag number\")\nplt.ylabel(\"Number of times tag appeared\")\nplt.legend()\nplt.show()\nprint(len(tag_counts[0:100:5]), tag_counts[0:100:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:39.072341Z","iopub.execute_input":"2023-09-27T19:05:39.072771Z","iopub.status.idle":"2023-09-27T19:05:39.606534Z","shell.execute_reply.started":"2023-09-27T19:05:39.072735Z","shell.execute_reply":"2023-09-27T19:05:39.605253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#store the tags greater than 10k in one list\nlst_tag_gt_10k=tag_df[tag_df.Counts>10000].Tags\n#Print the length of the list\nprint('{} tags more than 10 k'.format(len(lst_tag_gt_10k)))\n# Store tags greater than 100K in one list\nlst_tags_gt_100k = tag_df[tag_df.Counts>100000].Tags\n#Print the length of the list.\nprint ('{} Tags are used more than 100000 times'.format(len(lst_tags_gt_100k)))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:39.608461Z","iopub.execute_input":"2023-09-27T19:05:39.608918Z","iopub.status.idle":"2023-09-27T19:05:39.620478Z","shell.execute_reply.started":"2023-09-27T19:05:39.608882Z","shell.execute_reply":"2023-09-27T19:05:39.619155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Observations:</b><br />\n1. There are total 153 tags which are used more than 10000 times.\n2. 14 tags are used more than 100000 times.\n3. Most frequent tag (i.e. c#) is used 331505 times.\n4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm.","metadata":{}},{"cell_type":"markdown","source":"<h3> 2.2.4 Tags Per Question </h3>","metadata":{}},{"cell_type":"code","source":"#storing the count of tag in each question in list 'tag_count'\ntag_quest_count=tag_dtm.sum(axis=1).tolist()\n#converting lists of list into a single list\ntag_quest_count=[int(j) for i in tag_quest_count for j in i]\nprint('We have total {} datapoints'.format(len(tag_quest_count)))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:39.622355Z","iopub.execute_input":"2023-09-27T19:05:39.622903Z","iopub.status.idle":"2023-09-27T19:05:44.425048Z","shell.execute_reply.started":"2023-09-27T19:05:39.622854Z","shell.execute_reply":"2023-09-27T19:05:44.423687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( \"Maximum number of tags per question: %d\"%max(tag_quest_count))\nprint( \"Minimum number of tags per question: %d\"%min(tag_quest_count))\nprint( \"Avg. number of tags per question: %f\"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:44.427147Z","iopub.execute_input":"2023-09-27T19:05:44.427557Z","iopub.status.idle":"2023-09-27T19:05:44.619370Z","shell.execute_reply.started":"2023-09-27T19:05:44.427524Z","shell.execute_reply":"2023-09-27T19:05:44.618089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=tag_quest_count, palette='Set3')\nplt.title('Number of tags in the question')\nplt.xlabel('Number of tags')\nplt.ylabel('Number of questions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:05:44.621424Z","iopub.execute_input":"2023-09-27T19:05:44.622298Z","iopub.status.idle":"2023-09-27T19:06:03.518967Z","shell.execute_reply.started":"2023-09-27T19:05:44.622240Z","shell.execute_reply":"2023-09-27T19:06:03.518036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Observations:</b><br />\n1. Maximum number of tags per question: 5\n2. Minimum number of tags per question: 1\n3. Avg. number of tags per question: 2.899\n4. Most of the questions are having 2 or 3 tags","metadata":{}},{"cell_type":"markdown","source":"<h3>2.2.5 Most Frequent Tags </h3>","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n#plotting wordcloud\n\nstart=datetime.now()\n\n#converting result dictionary to list of tuples\n\ntup=dict(result.items())\n\nwordcloud=WordCloud(background_color='white',\n                   width=1600, height=1000).generate_from_frequencies(tup)\nfif=plt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\n#fig.savefig('tag.png')\nplt.show()\nprint('time taken: ', datetime.now()-start)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:06:03.520697Z","iopub.execute_input":"2023-09-27T19:06:03.522653Z","iopub.status.idle":"2023-09-27T19:06:10.040540Z","shell.execute_reply.started":"2023-09-27T19:06:03.522576Z","shell.execute_reply":"2023-09-27T19:06:10.039020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Observations:</b><br />\nA look at the word cloud shows that \"c#\", \"java\", \"php\", \"asp.net\", \"javascript\", \"c++\" are some of the most frequent tags.","metadata":{}},{"cell_type":"markdown","source":"<h3> 2.2.6 The top 20 tags </h3>","metadata":{}},{"cell_type":"code","source":"i = np.arange(30)\n\n# Plot the bar chart\ntop_30_tags = tag_df_sorted['Tags'].head(30)\ntop_30_counts = tag_df_sorted['Counts'].head(30)\nplt.bar(i, top_30_counts)\nplt.title('Frequency of top 30 Tags')\nplt.xticks(i, top_30_tags, rotation=90) \nplt.xlabel('Tags')\nplt.ylabel('Counts')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:06:10.043331Z","iopub.execute_input":"2023-09-27T19:06:10.044457Z","iopub.status.idle":"2023-09-27T19:06:10.632687Z","shell.execute_reply.started":"2023-09-27T19:06:10.044400Z","shell.execute_reply":"2023-09-27T19:06:10.631734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Observations:</b><br />\n1. Majority of the most frequent tags are programming language.\n2. C# is the top most frequent programming language.\n3. Android, IOS, Linux and windows are among the top most frequent operating systems.\n\n<h3> 2.3 Cleaning and preprocessing of Questions </h3>\n\n<h3> 2.3.1 Preprocessing </h3>\n\n<ol> \n    <li> Sample 1M data points </li>\n    <li> Separate out code-snippets from Body </li>\n    <li> Remove Spcial characters from Question title and description (not in code)</li>\n    <li> Remove stop words (Except 'C') </li>\n    <li> Remove HTML Tags </li>\n    <li> Convert all the characters into small letters </li>\n    <li> Use SnowballStemmer to stem the words </li>\n</ol>","metadata":{}},{"cell_type":"code","source":"def striphtml(data):\n    cleanr=re.compile('<.*?>')\n    cleantext=re.sub(cleanr,\" \",str(data))\n    return cleantext\nstop_words=set(stopwords.words('english'))\nstemmer=SnowballStemmer('english')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:06:10.634752Z","iopub.execute_input":"2023-09-27T19:06:10.635511Z","iopub.status.idle":"2023-09-27T19:06:10.643941Z","shell.execute_reply.started":"2023-09-27T19:06:10.635457Z","shell.execute_reply":"2023-09-27T19:06:10.642652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_connection(db_file):\n    try:\n        conn=sqlite3.connect(db_file)\n        return conn\n    except Error as e:\n        print(e)\n    return None\n\ndef create_table(conn,create_table_sql):\n    try:\n        c=conn.cursor()\n        c.execute(create_table_sql)\n    except Error as e:\n        print(e)\n\n\ndef checkTableExists(dbcon):\n    cursr=dbcon.cursor()\n    str=\"SELECT name FROM sqlite_master WHERE type='table'\"\n    table_names=cursr.execute(str)\n    print(\"tables in the database:\")\n    tables=table_names.fetchall()\n    print(tables[0][0])\n    return(len(tables))\n\ndef create_database_table(database,query):\n    conn=create_connection(database)\n    if conn is not None:\n        create_table(conn,query)\n        checkTableExists(conn)\n    else:\n        print(\"Error! cannot create database connection\")\n        conn.close()\n        \nsql_create_table=\"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\ncreate_database_table(\"Processed.db\", sql_create_table)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:06:10.646158Z","iopub.execute_input":"2023-09-27T19:06:10.647140Z","iopub.status.idle":"2023-09-27T19:06:10.663037Z","shell.execute_reply.started":"2023-09-27T19:06:10.647094Z","shell.execute_reply":"2023-09-27T19:06:10.661725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start=datetime.now()\nread_db='train_no_dup.db'\nwrite_db='Processed.db'\n\nif os.path.isfile(read_db):\n    conn_r=create_connection(read_db)\n    if conn_r is not None:\n        reader=conn_r.cursor()\n        reader.execute(\"SELECT Title, Body, Tags \\\n        FROM no_dup_train \\\n        ORDER BY RANDOM() LIMIT 1000000;\")\n        \nif os.path.isfile(write_db):\n    conn_w=create_connection(write_db)\n    if conn_w is not None:\n        tables= checkTableExists(conn_w)\n        writer=conn_w.cursor()\n        if tables!=0:\n            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n            print(\"cleared All the rows\")\nprint(\"time taken :\", datetime.now()-start)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:06:10.665809Z","iopub.execute_input":"2023-09-27T19:06:10.666223Z","iopub.status.idle":"2023-09-27T19:09:48.576109Z","shell.execute_reply.started":"2023-09-27T19:06:10.666174Z","shell.execute_reply":"2023-09-27T19:09:48.574170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__we create a new data base to store the sampled and preprocessed questions__","metadata":{}},{"cell_type":"code","source":"start=datetime.now()\npreprocessed_data_list=[]\nreader.fetchone()\nquestions_with_code=0\nlen_pre=0\nlen_post=0\nquestions_processed=0\n\nfor row in reader:\n    is_code=0\n    title,question,tags=row[0],row[1],row[2]\n    \n    if '<code>' in question:\n        questions_with_code+=1\n        is_code=1\n    x=len(question)+len(title)\n    len_pre+=x\n    \n    code=str(re.findall(r'<code>(.*?)</code>',question,flags=re.DOTALL))\n    \n    question=re.sub('<code>(.*?)</code>','',question,flags=re.MULTILINE|re.DOTALL)\n    question=striphtml(question.encode('utf-8'))\n    \n    question=str(title)+\" \"+str(question)\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    \n    len_post+=len(question)\n    tup=(question,code,tags,x,len(question),is_code)\n    questions_processed+=1\n    writer.execute(\"INSERT into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n    if(questions_processed%100000==0):\n        print('Number of questions processed =',questions_processed)\n        \n        \n        \nif questions_processed > 0:\n    no_dup_avg_len_pre = len_pre / questions_processed\n    no_dup_avg_len_post = len_post / questions_processed\n    print(\"Avg. length of questions (Title+Body) before processing: %d\" % no_dup_avg_len_pre)\n    print(\"Avg. length of questions (Title+Body) after processing: %d\" % no_dup_avg_len_post)\nelse:\n    print(\"No questions processed, so cannot calculate averages.\")\n\nprint(\"Time taken to run this cell :\", datetime.now() - start)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:09:48.578118Z","iopub.execute_input":"2023-09-27T19:09:48.578524Z","iopub.status.idle":"2023-09-27T19:52:28.694193Z","shell.execute_reply.started":"2023-09-27T19:09:48.578488Z","shell.execute_reply":"2023-09-27T19:52:28.691750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conn_r.commit()\nconn_w.commit()\nconn_r.close()\nconn_w.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:52:28.697326Z","iopub.execute_input":"2023-09-27T19:52:28.697803Z","iopub.status.idle":"2023-09-27T19:52:28.984372Z","shell.execute_reply.started":"2023-09-27T19:52:28.697747Z","shell.execute_reply":"2023-09-27T19:52:28.983199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile(write_db):\n    conn_r=create_connection(write_db)\n    if conn_r is not None:\n        reader=conn_r.cursor()\n        reader.execute(\"SELECT question\\\n        FROM QuestionsProcessed\\\n        LIMIT 10\")\n        print(\"Questions after preprocessing\")\n        print('+'*100)\n        reader.fetchone()\n        for row in reader:\n            print(row)\n            print('-'*100)\nconn_r.commit()\nconn_r.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:52:28.985989Z","iopub.execute_input":"2023-09-27T19:52:28.986358Z","iopub.status.idle":"2023-09-27T19:52:28.999191Z","shell.execute_reply.started":"2023-09-27T19:52:28.986322Z","shell.execute_reply":"2023-09-27T19:52:28.997528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Taking 1M entries to a df\nstart=datetime.now()\nwrite_db='Processed.db'\nif os.path.isfile(write_db):\n    conn_r=create_connection(write_db)\n    if conn_r is not None:\n        preprocessed_data=pd.read_sql_query(\"\"\"SELECT question, Tags\\\n                                                FROM QuestionsProcessed\"\"\",conn_r)\n        \nconn_r.commit()\nconn_r.close()\nprint('done : ',datetime.now()-start)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:52:29.001916Z","iopub.execute_input":"2023-09-27T19:52:29.002271Z","iopub.status.idle":"2023-09-27T19:52:33.674533Z","shell.execute_reply.started":"2023-09-27T19:52:29.002242Z","shell.execute_reply":"2023-09-27T19:52:33.670825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:52:33.679909Z","iopub.execute_input":"2023-09-27T19:52:33.681109Z","iopub.status.idle":"2023-09-27T19:52:33.734699Z","shell.execute_reply.started":"2023-09-27T19:52:33.680968Z","shell.execute_reply":"2023-09-27T19:52:33.730755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of data points in sample :\", preprocessed_data.shape[0])\nprint(\"number of dimensions :\", preprocessed_data.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T19:52:33.740481Z","iopub.execute_input":"2023-09-27T19:52:33.743715Z","iopub.status.idle":"2023-09-27T19:52:33.760120Z","shell.execute_reply.started":"2023-09-27T19:52:33.743652Z","shell.execute_reply":"2023-09-27T19:52:33.758338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}